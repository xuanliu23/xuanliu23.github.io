# Model Programming

​	本项目旨在探索多函数回归问题中神经网络的拟合与泛化能力。通过构建包含多种数学函数（如多项式、三角、指数、对数、分段等）的数据集，利用多层感知机（MLP）模型对这些函数进行联合学习，系统考察模型在同时拟合多种函数时的表现。在训练过程中，模型不仅学习了训练集中的所有函数，还通过最小二乘法将训练函数的输出线性组合，动态拟合测试集中的每一个函数。通过动画可视化，直观展示了模型在训练过程中的收敛情况及对不同测试函数的拟合能力。该项目为分析神经网络在多任务回归、抗噪声训练和泛化能力等方面提供了实验基础和可视化手段。



## 一、训练数据集准备

​	本项目所用的训练数据集由多种典型数学函数在区间 $[-5, 5]$ 上均匀采样200个点得到，涵盖了多项式、三角、指数、对数、分段等不同类型的函数。为提升模型的稳健性和泛化能力，所有训练集函数的输出均叠加了均值为0、标准差为0.01的高斯噪声作为微小扰动。这样的参数设置既保证了数据的多样性和代表性，又有效增强了模型对输入扰动的适应能力。每个训练样本由一个自变量 $x$ 及其对应的所有目标函数值组成，构成多输出回归任务的数据格式。

​	数据集中具体使用到的函数如下：$y_1 = 2x$，$y_2 = x^3$，$y_3 = 3x^2 + 3$，$y_4 = \dfrac{1}{x}$，$y_5 = x + \dfrac{1}{x}$，$y_6 = \sin(x)$，$y_7 = \cos(x)$，$y_8 = x^2$，$y_9 = x^4$，$y_{10} = e^{x}$，$y_{11} = e^{-x}$，$y_{12} = \log|x|$，$y_{13} = |x|$，$y_{14} = \tanh(x)$，$y_{15} = \dfrac{1}{1 + e^{-x}}$，$y_{16} = 1(x > 0 ),0 (x < 0)$，$y_{17} = \mathrm{sign}(x)$，$y_{18} = \sin(2x)$，$y_{19} = \cos(2x)$，$y_{20} = \sin(x)\cos(x)$，$y_{21} = \sqrt{|x|}$，$y_{22} = \arctan(x)$，$y_{23} = \arcsin\left(\dfrac{x}{5}\right)$，$y_{24} = \arccos\left(\dfrac{x}{5}\right)$，$y_{25} = \lfloor x \rfloor$，$y_{26} = \lceil x \rceil$，$y_{27} = x - \lfloor x \rfloor$，$y_{28} = x \bmod 3$，$y_{29} = H(x)$，$y_{30} = \max(0, x)$，$y_{31} = x(x > 0), 0.01x(x<0)$，$y_{32} = \log(1 + e^{x})$，$y_{33} = \cosh(x)$，$y_{34} = \sinh(x)$。

​	构建训练数据集的相关代码具体如下：

```Python
x = np.linspace(-5, 5, 200)
x_nozero = x.copy()
x_nozero[x_nozero == 0] = 1e-6
x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(1)
all_funcs = {
    'y_2x': 2 * x,
    'y_x3': x ** 3,
    'y_3x2_3': 3 * x ** 2 + 3,
    'y_1_x': 1 / x_nozero,
    'y_x_1_x': x + 1 / x_nozero,
    'y_sinx': np.sin(x),
    'y_cosx': np.cos(x),
    'y_x2': x ** 2,
    'y_x4': x ** 4,
    'y_exp_x': np.exp(x),
    'y_exp_negx': np.exp(-x),
    'y_log_absx': np.log(np.abs(x_nozero)),
    'y_absx': np.abs(x),
    'y_tanhx': np.tanh(x),
    'y_sigmoid': 1 / (1 + np.exp(-x)),
    'y_step': (x > 0).astype(float),
    'y_sign': np.sign(x),
    'y_sin2x': np.sin(2 * x),
    'y_cos2x': np.cos(2 * x),
    'y_sinx_cosx': np.sin(x) * np.cos(x),
    'y_sqrt_absx': np.sqrt(np.abs(x)),
    'y_arctanx': np.arctan(x),
    'y_arcsinx': np.arcsin(np.clip(x / 5, -1, 1)),
    'y_arccosx': np.arccos(np.clip(x / 5, -1, 1)),
    'y_floorx': np.floor(x),
    'y_ceilx': np.ceil(x),
    'y_fracx': x - np.floor(x),
    'y_mod3': np.mod(x, 3),
    'y_heaviside': np.heaviside(x, 0.5),
    'y_relu': np.maximum(0, x),
    'y_leakyrelu': np.where(x > 0, x, 0.01 * x),
    'y_softplus': np.log1p(np.exp(x)),
    'y_coshx': np.cosh(x),
    'y_sinhx': np.sinh(x),
}
# 为所有函数添加微小扰动
np.random.seed(123) 
data_train = {}
for name, arr in all_funcs.items():
    noise = np.random.normal(0, 0.01, x.shape)
    data_train[name] = arr + noise
y_train_tensor = torch.tensor(
    np.stack([data_train[name] for name in train_func_names], axis=1),
    dtype=torch.float32
)
dataset = torch.utils.data.TensorDataset(x_tensor, y_train_tensor)
train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)
```



## 二、构建神经网络

​	本项目使用深度学习框架**PyTorch**构建神经网络，所采用的神经网络为典型的**前馈神经网络**（Feedforward Neural Network），也称为多层感知机（MLP, Multi-Layer Perceptron）。该网络结构包含一个输入层、两层隐藏层和一个输出层，其中输入层的特征数量为` n_inputs=1`，即每次输入一个标量 $x$。每个隐藏层包含128个神经元，均采用ReLU激活函数以增强网络的非线性表达能力。输出层的神经元数量等于训练集中目标函数的数量，实现多输出回归。网络的训练过程中，采用批量大小为 `batch_size=64 `的小批量梯度下降，每次参数更新的步长由学习率 `lr=0.01`控制，整个训练过程共进行 `n_epochs=1000`轮。训练时，网络通过最小化均方误差损失（MSELoss），利用Adam优化器不断调整参数，使输出尽可能拟合所有训练函数。此外，训练过程中每隔 `record_interval=20` 轮会记录一次模型的预测结果，用于后续的分析和动态可视化。这样的结构和参数设置能够有效捕捉多种函数的复杂关系，实现对多目标回归任务的联合建模和泛化。

​	构建神经网络的相关代码具体如下：

```Python
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation
import torch
import torch.nn as nn
import torch.optim as optim

n_inputs = 1
n_epochs = 1000
batch_size = 64
lr = 0.01
record_interval = 20

# 神经网络
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(n_inputs, 128)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, n_outputs)
    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = self.relu(x)
        x = self.fc3(x)
        return x
net = Net()
```



## 三、训练神经网络

​	神经网络的训练过程采用均方误差损失函数（`nn.MSELoss`），以衡量模型输出与真实标签之间的差异，并使用Adam优化器（`optim.Adam`）对网络参数进行迭代优化，学习率设置为`lr=0.01`。在每个训练轮次（`epoch`，共`n_epochs=1000`轮）中，训练数据被分批（`batch_size=64`）送入网络进行前向传播，计算损失后通过反向传播（`loss.backward()`）和优化器步进（`optimizer.step()`）更新参数。训练过程中，每隔`record_interval=20`轮会记录一次当前网络对全部输入的预测结果和损失值，便于后续分析和动态可视化。每训练100轮还会输出一次当前损失，帮助监控模型收敛情况。通过这种方式，网络能够逐步调整参数，使输出尽可能逼近所有目标函数的真实值，实现多目标回归任务的有效拟合。

​	训练神经网络的相关代码具体如下：

```Python
criterion = nn.MSELoss()
optimizer = optim.Adam(net.parameters(), lr=lr)

predictions = []
losses = []
for epoch in range(n_epochs):
    for inputs, labels in train_loader:
        optimizer.zero_grad()
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
    if (epoch+1) % record_interval == 0 or epoch == 0:
        with torch.no_grad():
            y_pred = net(x_tensor).numpy() 
            predictions.append(y_pred)
            losses.append(loss.item())
    if (epoch+1) % 100 == 0:
        print(f"Epoch {epoch+1}/{n_epochs}, Loss: {loss.item():.6f}")
```



## 四、评估和测试

​	在测试过程中，首先使用的数据集为与训练集相同但未加扰动的“干净”函数集，确保能够客观评估模型的稳健性。每个测试函数在区间 $[-5, 5]$ 上均匀采样200个点，构成完整的测试输入和目标输出。测试时，模型对所有输入 $x$ 生成训练函数的预测输出，通过最小二乘法将这些输出线性组合，拟合每一个测试函数的真实值，并计算均方误差（MSE）作为性能指标。可视化部分采用Matplotlib的动画功能，动态展示训练过程中模型对指定测试函数的拟合曲线随训练轮数的变化情况。每一帧显示当前训练轮数、损失值和MSE，并将模型预测曲线与真实函数曲线进行对比，直观反映模型拟合能力的提升过程。

​	测试神经网络的相关代码具体如下：

```Python
data_test = {
    name: arr for name, arr in all_funcs.items()
}
test_func_names = list(data_test.keys())
n_outputs = len(train_func_names)
y_test_tensor = torch.tensor(
    np.stack([data_test[name] for name in test_func_names], axis=1),
    dtype=torch.float32
)

#动态可视化（以第0个函数为例）
test_func_idx = 0
test_func_name = test_func_names[test_func_idx]
y_true = data_test[test_func_name]

fig, ax = plt.subplots()
line_pred, = ax.plot([], [], 'b-', label='MLP combination prediction')
line_true, = ax.plot(x, y_true, 'r--', label='True function')
scat = ax.scatter(x, y_true, s=10, label='Test data', alpha=0.3)
ax.legend()
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_title(f'Test Function: {test_func_name}')

def init():
    line_pred.set_data([], [])
    return line_pred,

def update(frame):
    y_train_pred = predictions[frame] 
    coef, _, _, _ = np.linalg.lstsq(y_train_pred, y_true, rcond=None)
    y_pred = y_train_pred @ coef
    line_pred.set_data(x, y_pred)
    mse = np.mean((y_pred - y_true) ** 2)
    ax.set_title(f'Test Function: {test_func_name}\nEpoch: {frame*record_interval+1}, Loss: {losses[frame]:.2f}, MSE: {mse:.4f}')
    return line_pred,

ani = FuncAnimation(fig, update, frames=len(predictions), init_func=init, blit=True)
ani.save(r'C:\Users\xuan-\Desktop\test_func_training.gif', writer='pillow')
plt.show()
```

​	下图展示了部分函数（$y_2 = x^3$，$y_3 = 3x^2 + 3$，$y_6 = \sin(x)$，$y_{10} = e^{x}$，$y_{13} = |x|$，$y_{26} = \lceil x \rceil$）的拟合结果。可以发现：神经网络在拟合简单函数时表现十分优异，但对于一些复杂非线性函数以及不连续函数的拟合仍存在一定的不足。

<img src="https://github.com/xuanliu23/public001/blob/main/picture/test_func_training_1.gif?raw=true" alt="test_func_training_1.gif" style="zoom: 50%;" />

<img src="https://github.com/xuanliu23/public001/blob/main/picture/test_func_training_2.gif?raw=true" alt="test_func_training_2.gif" style="zoom:50%;" />

<img src="https://github.com/xuanliu23/public001/blob/main/picture/test_func_training_5.gif?raw=true" alt="test_func_training_5.gif" style="zoom:50%;" />

<img src="https://github.com/xuanliu23/public001/blob/main/picture/test_func_training_9.gif?raw=true" alt="![test_func_training_12.gif](https://github.com/xuanliu23/public001/blob/main/picture/test_func_training_12.gif?raw=true)" style="zoom:50%;" />

<img src="https://github.com/xuanliu23/public001/blob/main/picture/test_func_training_12.gif?raw=true" alt="test_func_training_12.gif" style="zoom:50%;" />

<img src="https://github.com/xuanliu23/public001/blob/main/picture/test_func_training_25.gif?raw=true" alt="test_func_training_25.gif" style="zoom:50%;" />

​	为了进一步测试神经网络的抗干扰能力以及模型的稳健性，本人在测试集的每个函数添加了均值为0、标准差为0.01的高斯噪声作为微小扰动，并使用了与训练集不同的随机种子以确保扰动的独立性。这样做的原因是为了让测试集的数据分布更加贴近真实场景下的噪声情况，从而更全面地评估模型的泛化能力。相比于直接用无扰动的理想函数，带有扰动的测试集可以有效避免模型对训练集噪声的过拟合，使得测试结果更具参考价值和实际意义。

​	测试集的构建代码调整如下：

```Python
np.random.seed(456)  
data_test = {}
for name, arr in all_funcs.items():
    noise = np.random.normal(0, 0.01, x.shape)
    data_test[name] = arr + noise
```

​	下图展示了$y_2 = x^3$，$y_3 = 3x^2 + 3$，$y_6 = \sin(x)$，$y_{10} = e^{x}$，$y_{13} = |x|$，$y_{26} = \lceil x \rceil$几个函数的拟合结果。可以发现：添加微小扰动前后模型的表现大体一致，表明模型具有较好的稳健性。

<img src="https://github.com/xuanliu23/public001/blob/main/picture/test_func_test_1.gif?raw=true" alt="test_func_test_1.gif" style="zoom:50%;" />

<img src="https://github.com/xuanliu23/public001/blob/main/picture/test_func_test_2.gif?raw=true" alt="test_func_test_2.gif" style="zoom:50%;" />

<img src="https://github.com/xuanliu23/public001/blob/main/picture/test_func_test_5.gif?raw=true" alt="test_func_test_5.gif" style="zoom:50%;" />

<img src="https://github.com/xuanliu23/public001/blob/main/picture/test_func_test_9.gif?raw=true" alt="test_func_test_9.gif" style="zoom:50%;" />

<img src="https://github.com/xuanliu23/public001/blob/main/picture/test_func_test_12.gif?raw=true" alt="test_func_test_12.gif" style="zoom:50%;" />

<img src="https://github.com/xuanliu23/public001/blob/main/picture/test_func_test_25.gif?raw=true" alt="test_func_test_25.gif" style="zoom:50%;" />



## 五、使用网络进行预测

​	为了进一步评估神经网络的泛化能力以及对更大范围内函数值的预测，本人使用与训练集相同但未加扰动的“干净”函数集，并使用训练过的神经网络对函数$[-7, 7]$ 区间范围内的值进行预测。每个测试函数在区间 $[-7, 7]$ 上均匀采样300个点，构成完整的测试输入和目标输出。         

​	相关代码具体如下：

```Python
x_new = np.linspace(-7, 7, 300)
x_new_nozero = x_new.copy()
x_new_nozero[x_new_nozero == 0] = 1e-6
test_inputs = torch.tensor(x_new, dtype=torch.float32).unsqueeze(1)
all_funcs_new = {
    'y_2x': 2 * x_new,
    'y_x3': x_new ** 3,
    'y_3x2_3': 3 * x_new ** 2 + 3,
    'y_1_x': 1 / x_new_nozero,
    'y_x_1_x': x_new + 1 / x_new_nozero,
    'y_sinx': np.sin(x_new),
    'y_cosx': np.cos(x_new),
    'y_x2': x_new ** 2,
    'y_x4': x_new ** 4,
    'y_exp_x': np.exp(x_new),
    'y_exp_negx': np.exp(-x_new),
    'y_log_absx': np.log(np.abs(x_new_nozero)),
    'y_absx': np.abs(x_new),
    'y_tanhx': np.tanh(x_new),
    'y_sigmoid': 1 / (1 + np.exp(-x_new)),
    'y_step': (x_new > 0).astype(float),
    'y_sign': np.sign(x_new),
    'y_sin2x': np.sin(2 * x_new),
    'y_cos2x': np.cos(2 * x_new),
    'y_sinx_cosx': np.sin(x_new) * np.cos(x_new),
    'y_sqrt_absx': np.sqrt(np.abs(x_new)),
    'y_arctanx': np.arctan(x_new),
    'y_arcsinx': np.arcsin(np.clip(x_new / 5, -1, 1)),
    'y_arccosx': np.arccos(np.clip(x_new / 5, -1, 1)),
    'y_floorx': np.floor(x_new),
    'y_ceilx': np.ceil(x_new),
    'y_fracx': x_new - np.floor(x_new),
    'y_mod3': np.mod(x_new, 3),
    'y_heaviside': np.heaviside(x_new, 0.5),
    'y_relu': np.maximum(0, x_new),
    'y_leakyrelu': np.where(x_new > 0, x_new, 0.01 * x_new),
    'y_softplus': np.log1p(np.exp(x_new)),
    'y_coshx': np.cosh(x_new),
    'y_sinhx': np.sinh(x_new),
}

with torch.no_grad():
    y_pred_new = net(test_inputs).numpy()  

#可视化（以第0个函数为例）
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation

test_func_idx = 0
test_func_name = list(all_funcs_new.keys())[test_func_idx]
y_true_new = all_funcs_new[test_func_name]

fig, ax = plt.subplots()
line_pred, = ax.plot([], [], 'b-', label='MLP prediction')
line_true, = ax.plot(x_new, y_true_new, 'r--', label='True function')
ax.legend()
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_title(f'Generalization: {test_func_name}')

def init():
    line_pred.set_data([], [])
    return line_pred,

def update(frame):
    y_pred = y_pred_new[:, test_func_idx]
    line_pred.set_data(x_new, y_pred)
    return line_pred,

ani = FuncAnimation(fig, update, frames=1, init_func=init, blit=True)
ani.save(r'C:\Users\xuan-\Desktop\test_func_new_0.gif', writer='pillow')
plt.show()
```

​	下图展示了$y_1 = 2x$，$y_2 = x^3$，$y_3 = 3x^2 + 3$，$y_6 = \sin(x)$，$y_{10} = e^{x}$，$y_{13} = |x|$几个函数的拟合结果。可以发现：对于线性函数$y_1 = 2x$，神经网络对于原区间展现出较好的拟合效果，而在泛化区间拟合曲线较原函数略有偏移；对于二次函数$y_3 = 3x^2 + 3$，神经网络展现出的拟合效果最佳，在原区间和泛化区间拟合函数与原函数重叠度均较高；对于三次函数$y_2 = x^3$和指数函数$y_{10} = e^{x}$，神经网络在原区间的拟合效果很好，但是由于泛化区间函数增长速度迅速加大，神经网络对于泛化区间的拟合较原函数略有偏移；对于三角函数$y_6 = \sin(x)$和绝对值函数$y_{13} = |x|$，由于图像存在拐点，导致神经网络对于泛化区间内的预测与实际出现较大偏差 ，需要增大训练集的区间范围来实现进一步的拟合。

<img src="https://github.com/xuanliu23/public001/blob/main/picture/test_func_new_0.gif?raw=true" alt="test_func_new_0.gif" style="zoom:50%;" />

<img src="https://github.com/xuanliu23/public001/blob/main/picture/test_func_new_1.gif?raw=true" alt="test_func_new_1.gif" style="zoom:50%;" />

<img src="https://github.com/xuanliu23/public001/blob/main/picture/test_func_new_2.gif?raw=true" alt="test_func_new_2.gif" style="zoom:50%;" />

<img src="https://github.com/xuanliu23/public001/blob/main/picture/test_func_new_5.gif?raw=true" alt="test_func_new_5.gif" style="zoom:50%;" />

<img src="https://github.com/xuanliu23/public001/blob/main/picture/test_func_new_9.gif?raw=true" alt="test_func_new_9.gif" style="zoom:50%;" />

<img src="https://github.com/xuanliu23/public001/blob/main/picture/test_func_new_12.gif?raw=true" alt="test_func_new_12.gif" style="zoom:50%;" />



## 总结

​	本项目探索了多函数回归问题中神经网络的拟合与泛化能力，通过构建包含多种典型数学函数的多输出数据集，利用多层感知机模型实现了对复杂函数族的联合建模与动态可视化分析。实验结果表明，神经网络能够较好地拟合大部分连续、平滑的函数，并在一定程度上具备抗噪声和泛化能力，但对于区间外的极端值、剧烈变化或不连续函数的拟合仍存在一定局限，仍需进一步完善。


